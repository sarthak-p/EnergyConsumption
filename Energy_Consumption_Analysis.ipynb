{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8811277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3400: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3400: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3400: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3400: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3400: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3400: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3400: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Parameters: {'subsample': 0.8315789473684211, 'n_estimators': 144, 'max_depth': 6, 'learning_rate': 0.04657894736842105, 'gamma': 0.3342105263157895, 'colsample_bytree': 1.0}\n",
      "Best XGBoost Score: 0.3374239468956142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthak/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 30 is smaller than n_iter=100. Running 30 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RandomForest Parameters: {'n_estimators': 240, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_depth': 21, 'bootstrap': True}\n",
      "Best RandomForest Score: 0.3622693455981518\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fiona\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load data file\n",
    "df = pd.read_csv('comed_201910.csv')\n",
    "\n",
    "# Filter out only residential accounts\n",
    "df_residential = df[df['service_name'].str.contains('residential', case=False, na=False)]\n",
    "\n",
    "# Filter for one day, ex. 2019-10-01\n",
    "desired_date = '2019-10-01'\n",
    "df_residential = df_residential[df_residential['date_time'].str.contains(desired_date)]\n",
    "\n",
    "# Convert the time from HH:MM to seconds past since the beginning of the day\n",
    "df_residential['date_time'] = pd.to_datetime(df_residential['date_time'])\n",
    "df_residential['seconds_past'] = df_residential['date_time'].dt.hour * 3600 + df_residential['date_time'].dt.minute * 60\n",
    "\n",
    "# Aggregate data to find time of maximum energy consumption for each zip code\n",
    "df_grouped = df_residential.groupby('zip5')['energy'].idxmax()\n",
    "peak_times = df_residential.loc[df_grouped, ['zip5', 'energy', 'seconds_past']]\n",
    "peak_times.columns = ['zip_code', 'peak_energy_value', 'peak_energy_time']\n",
    "\n",
    "# Load the geojson file\n",
    "gdf = gpd.read_file('Chicago_ZC.geojson')\n",
    "\n",
    "# Convert the 'GEOID20' column to integer for join operation\n",
    "gdf['GEOID20'] = gdf['GEOID20'].astype(int)\n",
    "\n",
    "# Merge the peak_times DataFrame with the gdf GeoDataFrame based on 'zip_code'\n",
    "gdf = gdf.merge(peak_times, left_on='GEOID20', right_on='zip_code', how='left')\n",
    "\n",
    "# Convert the geometries to a projected CRS\n",
    "gdf = gdf.to_crs('EPSG:3857')\n",
    "\n",
    "# Calculate the centroid of each polygon\n",
    "gdf['centroid'] = gdf['geometry'].centroid\n",
    "\n",
    "# Convert back to geographic for lat/long coordinates\n",
    "gdf = gdf.to_crs('EPSG:4269')\n",
    "\n",
    "# Convert the centroid to latitude and longitude coordinates\n",
    "gdf['Lat_centroid'] = gdf['centroid'].y\n",
    "gdf['Long_centroid'] = gdf['centroid'].x\n",
    "\n",
    "# Keep only the desired columns\n",
    "gdf = gdf[['zip_code', 'peak_energy_value', 'peak_energy_time', 'Lat_centroid', 'Long_centroid']]\n",
    "\n",
    "# Convert 'zip_code' to int, handling NaN values which cannot be converted to int\n",
    "gdf['zip_code'] = gdf['zip_code'].astype('Int64')\n",
    "\n",
    "gdf = gdf.dropna()\n",
    "\n",
    "# Convert back to GeoDataFrame\n",
    "gdf['geometry'] = gpd.points_from_xy(gdf.Long_centroid, gdf.Lat_centroid)\n",
    "gdf = gpd.GeoDataFrame(gdf, geometry='geometry')\n",
    "gdf.set_crs(\"EPSG:4269\", inplace=True)\n",
    "\n",
    "# Load the land use shapefile into a GeoDataFrame\n",
    "gdb_folder = 'Landuse2018_CMAP_v1.gdb'\n",
    "\n",
    "# Function to load the data in chunks, using only the necessary columns\n",
    "def load_data(gdb_folder, layer):\n",
    "    # Residential land use codes\n",
    "    residential_codes = ['1000', '1100', '1110', '1111', '1112', '1130', '1140', '1150', '1151']\n",
    "    \n",
    "    with fiona.Env():\n",
    "        with fiona.open(gdb_folder, layer=layer) as src:\n",
    "            # Read all records\n",
    "            records = [{'geometry': f['geometry'], 'LANDUSE': f['properties']['LANDUSE']} \n",
    "                       for f in src if f is not None and 'geometry' in f]\n",
    "            \n",
    "            df = pd.DataFrame.from_records(records)\n",
    "            gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=src.crs)\n",
    "\n",
    "    # Filter the GeoDataFrame to include only desired land use categories\n",
    "    gdf = gdf[gdf['LANDUSE'].isin(residential_codes)]\n",
    "    \n",
    "    # Now split into chunks\n",
    "    gdfs = [gdf[i:i+chunksize] for i in range(0, gdf.shape[0], chunksize)]\n",
    "    return gdfs\n",
    "\n",
    "chunksize = 50000\n",
    "\n",
    "land_use = load_data(gdb_folder, 'Landuse2018_Dissolved_v1')\n",
    "\n",
    "print(\"yes0\")\n",
    "\n",
    "# Convert the land use data to the same CRS as the zip code data\n",
    "land_use = [lu_gdf.to_crs(gdf.crs) for lu_gdf in land_use]\n",
    "\n",
    "# Initialize an empty list to hold the results\n",
    "land_use_joined = []\n",
    "\n",
    "for lu_gdf in land_use:\n",
    "    # Perform a spatial join to associate each land use parcel with its corresponding zip code\n",
    "    joined = gpd.sjoin(lu_gdf, gdf[['geometry', 'zip_code']], how='left', op='within')\n",
    "    land_use_joined.append(joined)\n",
    "\n",
    "# Combine the results\n",
    "land_use = pd.concat(land_use_joined)\n",
    "\n",
    "land_use = land_use.to_crs('EPSG:3857')\n",
    "\n",
    "# Calculate the area of each land use type in each zip code\n",
    "land_use['area'] = land_use.geometry.area\n",
    "\n",
    "land_use = land_use.to_crs(gdf.crs)\n",
    "\n",
    "# Summarize land use area by land use type and zip code\n",
    "land_use_summary = land_use.groupby(['zip_code', 'LANDUSE']).area.sum().unstack().reset_index()\n",
    "\n",
    "# Fill NA values with 0 (no land of that type in the zip code)\n",
    "land_use_summary.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the total area of each zip code\n",
    "total_area = land_use_summary.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Normalize the land use areas by the total area of each zip code\n",
    "for col in land_use_summary.columns[1:]:\n",
    "    land_use_summary[col] = land_use_summary[col] / total_area\n",
    "\n",
    "# Join the land use data with the original DataFrame\n",
    "gdf = gdf.merge(land_use_summary, on='zip_code', how='left')\n",
    "\n",
    "# Load the data\n",
    "data = gdf\n",
    "\n",
    "# Split the data into X and Y variables\n",
    "# Include all columns that are not 'peak_energy_time' and 'geometry' as input features (including the new land use features)\n",
    "X = data.drop(columns=['peak_energy_time', 'geometry']) \n",
    "Y = data['peak_energy_time']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# *** THIS CODE USES RANDOMIZED SEARCH AND IS LESS COMPUTATIONALLY EXPENSIVE THAN GRID SEARCH *** \n",
    "\n",
    "# Refining hyperparameters for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': list(range(135, 145)),  # Focusing more on 140\n",
    "    'max_depth': [6],  # Optimal value from the last search\n",
    "    'learning_rate': np.linspace(0.045, 0.055, 20),  # Focusing more around 0.049\n",
    "    'gamma': np.linspace(0.33, 0.37, 20),  # Focusing more around 0.35\n",
    "    'colsample_bytree': [1.0],  # Optimal value from the last search\n",
    "    'subsample': np.linspace(0.80, 0.85, 20)  # Focusing more around 0.82\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "xgb_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=xgb_param_grid, n_iter=100, cv=5, random_state=42)\n",
    "xgb_search.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best XGBoost Parameters:\", xgb_search.best_params_)\n",
    "print(\"Best XGBoost Score:\", xgb_search.best_score_)\n",
    "\n",
    "# Refining hyperparameters for RandomForest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': list(range(235, 245)),  # Focusing more around 241\n",
    "    'max_depth': list(range(20, 23)),  # Focusing more around 21\n",
    "    'min_samples_split': [8],  # Optimal value from the last search\n",
    "    'min_samples_leaf': [1],  # Optimal value from the last search\n",
    "    'bootstrap': [True]  # Optimal value from the last search\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf_search = RandomizedSearchCV(estimator=rf_model, param_distributions=rf_param_grid, n_iter=100, cv=5, random_state=42)\n",
    "rf_search.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best RandomForest Parameters:\", rf_search.best_params_)\n",
    "print(\"Best RandomForest Score:\", rf_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
